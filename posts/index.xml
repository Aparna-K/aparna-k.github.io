<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Datascope</title>
    <link>https://aparna-k.com/posts/</link>
    <description>Recent content in Posts on Datascope</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Dec 2018 19:02:00 +0530</lastBuildDate>
    
	<atom:link href="https://aparna-k.com/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Maximum Entropy Model - The details (Part 2/3)</title>
      <link>https://aparna-k.com/posts/maxentmodelling/maxent-2/</link>
      <pubDate>Thu, 06 Dec 2018 19:02:00 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/maxentmodelling/maxent-2/</guid>
      <description>Model representation Output value: &amp;gt; $y_i \in Y$ &amp;gt; where $Y$ is a finite set ${y_1, y_2,&amp;hellip;,y_n}$
Let&amp;rsquo;s take for example, the translation model from my previous post
The task is to find the best translation of the word in to French. From the training data we infer that the translations of in to French typically is one of the following {dans, en, à, au cours de, pendant}
here, $Y = {dans, en, à, au cours de, pendant}$</description>
    </item>
    
    <item>
      <title>Maximum Entropy Model - Intuition (Part 1/3)</title>
      <link>https://aparna-k.com/posts/maxentmodelling/maxent-1/</link>
      <pubDate>Tue, 04 Dec 2018 19:02:00 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/maxentmodelling/maxent-1/</guid>
      <description>Principle of Maximum Entropy  &amp;ldquo;In making inferences on the basis of partial information we must use that probability distribution which has maximum entropy subject to whatever is known. This is the only unbiased assignment we can make; to use any other would amount to arbitrary assumption of information which,by hypothesis, we do not have&amp;rdquo; - [Jaynes, 1957]
 If you are unfamiliar with the term information entropy, refer to my previous post</description>
    </item>
    
    <item>
      <title>The idea of Information Entropy</title>
      <link>https://aparna-k.com/posts/informationtheoryentropy/information_entropy/</link>
      <pubDate>Sat, 01 Dec 2018 19:02:42 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/informationtheoryentropy/information_entropy/</guid>
      <description>Information Entropy  Information entropy quantifies the average amount of information in an event.
 We could also think of entropy as a measure of unpredictability.
Information - Intuition Let&amp;rsquo;s consider the following three sentences
 Tomorrow, the sun will rise from the East The phone will ring in the next one hour It will snow in Mumbai this winter  When we consider the amount of information in the three sentences above, there are two ways to think about it.</description>
    </item>
    
    <item>
      <title>PCA on MNIST using SKLearn - PCA Part 3/3</title>
      <link>https://aparna-k.com/posts/pca/pca_on_mnist/</link>
      <pubDate>Sun, 12 Mar 2017 17:32:42 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/pca/pca_on_mnist/</guid>
      <description>To understand basics of PCA, refer to my previous post here
Quick note regarding MNIST dataset: Ref: https://en.wikipedia.org/wiki/MNIST_database#Dataset
The MNIST dataset is a large database of handwritten digits that are commonly used for training and testing ML models.
The MNIST dataset in sklearn conatains 1797 digits. Each digit is represented by an array of 64 values that represent a 8x8 pixel image. Each pixel value ranges from 0-16.
import numpy as np import matplotlib.</description>
    </item>
    
    <item>
      <title>Principal Components Analysis - PCA Part 2/3</title>
      <link>https://aparna-k.com/posts/pca/pca/</link>
      <pubDate>Sat, 11 Mar 2017 19:59:42 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/pca/pca/</guid>
      <description>What is PCA? In simple words, PCA is a technique used to condense a large number of features into a smaller set of features that can almost represent the original set.
Why PCA?  PCA is used for dimensionality reduction.
 As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.
This is called &amp;ldquo;Curse of dimensionality&amp;rdquo;. Here is a video that explains this in simple terms.</description>
    </item>
    
    <item>
      <title>Covariance, Eigen Vectors and Principal Components - PCA Part 1/3</title>
      <link>https://aparna-k.com/posts/pca/covariance_eigen_pca/</link>
      <pubDate>Fri, 10 Mar 2017 19:59:42 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/pca/covariance_eigen_pca/</guid>
      <description>Covariance matrix: Here is a really good reference to build an intuition for covariance matrix
Also known as the variance-covariance matrix, holds information about how two random variables (in this case two features) vary with respect to one another
Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together.</description>
    </item>
    
  </channel>
</rss>