<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Datascope  | Principal Components Analysis - PCA Part 2/3</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.40.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="http://aparna-k.com/dist/css/app.ab4b67a3ea25990fa8279f3b7ef08b61.css" rel="stylesheet">
    

    

    
      
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />

    

    

    <meta property="og:title" content="Principal Components Analysis - PCA Part 2/3" />
<meta property="og:description" content="What is PCA? In simple words, PCA is a technique used to condense a large number of features into a smaller set of features that can almost represent the original set.
Why PCA?  PCA is used for dimensionality reduction.
 As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.
This is called &ldquo;Curse of dimensionality&rdquo;. Here is a video that explains this in simple terms." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://aparna-k.com/posts/pca/" />



<meta property="article:published_time" content="2017-03-11T19:59:42&#43;05:30"/>

<meta property="article:modified_time" content="2017-03-11T19:59:42&#43;05:30"/>











<meta itemprop="name" content="Principal Components Analysis - PCA Part 2/3">
<meta itemprop="description" content="What is PCA? In simple words, PCA is a technique used to condense a large number of features into a smaller set of features that can almost represent the original set.
Why PCA?  PCA is used for dimensionality reduction.
 As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.
This is called &ldquo;Curse of dimensionality&rdquo;. Here is a video that explains this in simple terms.">


<meta itemprop="datePublished" content="2017-03-11T19:59:42&#43;05:30" />
<meta itemprop="dateModified" content="2017-03-11T19:59:42&#43;05:30" />
<meta itemprop="wordCount" content="690">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Principal Components Analysis - PCA Part 2/3"/>
<meta name="twitter:description" content="What is PCA? In simple words, PCA is a technique used to condense a large number of features into a smaller set of features that can almost represent the original set.
Why PCA?  PCA is used for dimensionality reduction.
 As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.
This is called &ldquo;Curse of dimensionality&rdquo;. Here is a video that explains this in simple terms."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="http://aparna-k.com/" class="f3 fw2 hover-white no-underline white-90 dib">
      Datascope
    </a>
    <div class="flex-l items-center">
      
      








    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  <article class="flex-l flex-wrap justify-between mw8 center">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">Principal Components Analysis - PCA Part 2/3</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2017-03-11T19:59:42&#43;05:30">March 11, 2017</time>
    </header>

    <main class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">

<h2 id="what-is-pca">What is PCA?</h2>

<p>In simple words, PCA is a technique used to <em>condense</em> a large number of features into a smaller set of features that can <em>almost</em> represent the original set.</p>

<h2 id="why-pca">Why PCA?</h2>

<blockquote>
<p>PCA is used for dimensionality reduction.</p>
</blockquote>

<p>As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.</p>

<p>This is called <em>&ldquo;Curse of dimensionality&rdquo;</em>. <a href="https://www.youtube.com/watch?v=QZ0DtNFdDko">Here is a video</a> that explains this in simple terms.</p>

<p>PCA is a technique used to reduce diemnsions (or number of features).</p>

<h2 id="how-is-it-done">How is it done?</h2>

<p>For the sake of simplicity, consider a set of two features X = { X<sub>1</sub>, X<sub>2</sub> }</p>

<p>We are trying to find a line Z onto which we can project the points in the plane formed by { X<sub>1</sub>, X<sub>2</sub> }</p>

<p>Below is a reasonable visual representation of the idea</p>

<p><img class="special-img-class" style="max-width:400px; max-height:450px" src="https://liorpachter.files.wordpress.com/2014/05/pca_figure1.jpg?w=490&h=490" /></p>

<p>The line Z is the the 1D representation of the 2D dataset.</p>

<h2 id="what-are-principal-components">What are principal components?</h2>

<p>Similar to the line Z, there are other such lines, orthogonal to each other onto which points in the space can be projected.</p>

<p>Each of these lines capture the spatial information with varying degrees of accuracy.</p>

<p>In the image below we want to pick e to maximize variability. i.e., the <span style="color:#5f5fff"> <strong>purple</strong> </span> line captures the variability among points better than the <span style="color:green"> <strong>green</strong> </span> line</p>

<p><a href='http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/dim-2x2.pdf'><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/pca/PCA1.png" /></a></p>

<p>We want to do this, so that we reduce cases where points that are far apart in the original space are very close together on the projection.</p>

<p>The set of orthogonal lines that capture most amount of variability in the dataset are known as <strong>principal components</strong>.</p>

<h2 id="calculating-principal-components">Calculating principal components</h2>

<blockquote>
<p>Principal components are the eigen vectors of the covariance matrix of the dataset with the largest eigen values.</p>

<p>The eigenvector with the largest eigenvalue is the direction along which the data set has the maximum variance</p>
</blockquote>

<p>Refer to my previous post on <a href="http://aparna-k.com/posts/covariance_eigen_pca/">covariance, eigen vectors and pca</a> for some background.</p>

<p>The technique of finding principal components is called <a href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca">&ldquo;Singular Value Decomposition (SVD)&rdquo;</a></p>

<p>However, we need not get into the details of SVD here, since for a symmetric matrix (which a covariance matrix is), <strong>singular values are just the absolute values of the eigenvalues of eigenvectors of a covariance matrix.</strong></p>

<h2 id="steps-involved-in-pca">Steps involved in PCA</h2>

<ol>
<li>Given a dataset, compute it&rsquo;s covariance matrix</li>
<li>Find the eigenvalues and eigenvectors for the covariance matrix</li>
<li>Sort the eigenvectors in descending order of their eigenvalues</li>
<li>Pick <strong>K</strong> eigenvectors from the sorted list (this set represents the maximum variance in data)</li>
</ol>

<h3 id="choosing-k-the-right-number-of-eigen-vectors">Choosing <strong>K</strong> (the right number of eigen vectors)</h3>

<p>Given a set of eigenvalues &lambda;<sub>1</sub>, &lambda;<sub>2</sub>, &lambda;<sub>3</sub> &hellip; &lambda;<sub>n</sub> in descending order of magnitude, pick &lambda;<sub>1</sub>, &lambda;<sub>2</sub>, &lambda;<sub>3</sub> &hellip; &lambda;<sub><b>K</b></sub> that <a href="https://ro-che.info/articles/2017-12-11-pca-explained-variance">&ldquo;explain the most variance&rdquo;</a></p>

<p>Pick <strong>K</strong> eigenvectors that explain 90% or more of the total variance, using the following equation</p>

<p><a href='http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/dim-2x2.pdf'><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/pca/explained_variance_pca.png" /></a></p>

<p>Typical threshold values are 0.9 or 0.95.
We could set a explained variance threshold as high as 0.99 if we want to capture the original data with higher accuracy in the compressed representation.</p>

<h2 id="reconstructing-data-from-pca">Reconstructing data from PCA:</h2>

<p>Let <strong>X<sub>raw</sub></strong> be the <em>n×p</em> data matrix with <em>n</em> rows (data points) and <em>p</em> columns (variables, or features). After subtracting the mean vector <em>μ</em> from each row, we get the centered data matrix <strong>X</strong>.</p>

<p>Let <strong>V</strong> be the <em>p×k</em> matrix of some <strong>K</strong> eigenvectors that we want to use; these would most often be the <strong>K</strong> eigenvectors with the largest eigenvalues. Then the <em>n×k</em> matrix of PCA projections (&ldquo;scores&rdquo;
will be simply given by <strong>Z=XV</strong>.</p>

<p>The reconstruction of the original data from Z is given by</p>

<p>X̂raw = ZVT + μ</p>

<h2 id="advice-for-applying-pca">Advice for applying PCA</h2>

<ol>
<li><p><strong>Perform feature scaling</strong> if the range of values for different features vary greatly.
Larger numbers are likely to have higher variance which can overwhelm the algorithm since we use PCs from the data&rsquo;s covariance matrix.</p></li>

<li><p><strong>Don&rsquo;t include PCA as a default step</strong> in an ML system design. Only apply PCA if the training doesn&rsquo;t work well with the original large feature set.</p></li>

<li><p><strong>PCA is a good way to reduce memory requirements</strong> and to speed up a learning algorithm</p></li>

<li><p><strong>PCA is not a good solution for overfitting</strong>. Instead of reducing the number of features using PCA, use regularization.</p></li>
</ol>
<ul class="pa0">
  
</ul>
<div class="mt6">
        
      </div>
    </main>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://aparna-k.com/" >
    &copy; 2018 Datascope
  </a>
  








  </div>
</footer>

    

  <script src="http://aparna-k.com/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
