<!DOCTYPE html>
<html lang="en-us">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.52" />

    
    
    

<title>Covariance, Eigen Vectors and Principal Components - PCA Part 1/3 • Datascope</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Covariance, Eigen Vectors and Principal Components - PCA Part 1/3"/>
<meta name="twitter:description" content="Covariance matrix: Here is a really good reference to build an intuition for covariance matrix
Also known as the variance-covariance matrix, holds information about how two random variables (in this case two features) vary with respect to one another
Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together."/>

<meta property="og:title" content="Covariance, Eigen Vectors and Principal Components - PCA Part 1/3" />
<meta property="og:description" content="Covariance matrix: Here is a really good reference to build an intuition for covariance matrix
Also known as the variance-covariance matrix, holds information about how two random variables (in this case two features) vary with respect to one another
Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aparna-k.com/posts/pca/covariance_eigen_pca/" /><meta property="article:published_time" content="2017-03-10T19:59:42&#43;05:30"/>
<meta property="article:modified_time" content="2017-03-10T19:59:42&#43;05:30"/>


    






<link rel="stylesheet" href="/scss/hyde-hyde.7edd83059122cd386342a3b1c68b1e7b1b05b9634400072d2221148b98ef3449.css" integrity="sha256-ft2DBZEizThjQqOxxoseexsFuWNEAActIiEUi5jvNEk=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href=favicon.ico>
    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://aparna-k.com/">Datascope</a>
      </span>
      
      
      <p class="site__description">
         Prediction is risky, especially when it involves the future. - Niels Bohr 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Datascope</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		
		</li>
	</ul>
</div>

        <section class="social">
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/aparnakorattyswaroopam" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
</section>

      </div>
    </div>
    <div class="copyright">
  2017
</div>

  </div>
</div>

        <div class="content container">
            
    <article>
  <header>
    <h1>Covariance, Eigen Vectors and Principal Components - PCA Part 1/3</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Mar 10, 2017
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 3 min read
</div>


  </header>
  
  
  <div class="post">
    

<h2 id="covariance-matrix">Covariance matrix:</h2>

<p><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/">Here is a really good reference</a> to build an intuition for covariance matrix</p>

<p>Also known as the variance-covariance matrix, holds information about how two random variables (in this case two features) vary with respect to one another</p>

<p>Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together.</p>

<p><a href="http://www.statisticshowto.com/covariance/"><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/pca/covariance.png"/></a></p>

<p>A covariance matrix summarizes the co-variances of a set of variables in a matrix</p>

<p><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/pca/covariance_matrix_formula.png" /></a></p>

<p>Where,</p>

<p><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/pca/covariance_formula.png" /></a></p>

<p>The operator <strong>E</strong> is the mean (expected value) of it&rsquo;s argument</p>

<p><a href="https://en.wikipedia.org/wiki/Covariance_matrix"><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/pca/mean.png" /></a></p>

<h5 id="shape-of-covariance-matrix">Shape of covariance matrix:</h5>

<p>The following figure illustrates the shape of data and their corresponding covariance matrix</p>

<p><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"><img class="special-img-class" src="/img/pca/shape_of_covariance.png" /></a></p>

<p>Understanding the shape:</p>

<p><em>Top-Left</em>:</p>

<p>&sigma;(x, x) = 5 -&gt; Variance of x w.r.t itself, i.e., variance of x <br>
&sigma;(x, y) = 4 -&gt; Variance of x w.r.t y <br>
&sigma;(y, x) = 4 -&gt; Variance of y w.r.t x <br>
&sigma;(y, y) = 6 -&gt; Variance of y w.r.t itself, i.e., variance of y <br></p>

<p>This implies that x, y have a positive covariance, i.e., x increases as y increases</p>

<p><em>Top-Right</em>:</p>

<p>&sigma;(x, x) = 5 -&gt; Variance of x w.r.t itself, i.e., variance of x <br>
&sigma;(x, y) = -4 -&gt; Variance of x w.r.t y <br>
&sigma;(y, x) = -4 -&gt; Variance of y w.r.t x <br>
&sigma;(y, y) = 6 -&gt; Variance of y w.r.t itself, i.e., variance of y <br></p>

<p>This implies that x, y have a negative covariance, i.e., x decreases as y increases and vice-versa</p>

<p><em>Bottom-Left</em>:</p>

<p>&sigma;(x, x) = 5 -&gt; Variance of x w.r.t itself, i.e., variance of x <br>
&sigma;(x, y) = 0 -&gt; Variance of x w.r.t y <br>
&sigma;(y, x) = 0 -&gt; Variance of y w.r.t x <br>
&sigma;(y, y) = 1 -&gt; Variance of y w.r.t itself, i.e., variance of y <br></p>

<p>This implies that x, y have a zero covariance, i.e., x and y have no relationship. Also the variance of x is much higher than y giving the shape that we see where the data is spread out maximally in the x direction</p>

<p><em>Bottom-Right</em>:</p>

<p>&sigma;(x, x) = 1 -&gt; Variance of x w.r.t itself, i.e., variance of x <br>
&sigma;(x, y) = 0 -&gt; Variance of x w.r.t y <br>
&sigma;(y, x) = 0 -&gt; Variance of y w.r.t x <br>
&sigma;(y, y) = 5 -&gt; Variance of y w.r.t itself, i.e., variance of y <br></p>

<p>This implies that x, y have a zero covariance, i.e., x and y have no relationship. Also the variance of y is much higher than x giving the shape that we see where the data is spread out maximally in the y direction</p>

<h3 id="eigenvectors-of-covariance-matrix">Eigenvectors of Covariance matrix:</h3>

<p>I highly recommend watching this <a href="https://www.youtube.com/watch?v=PFDu9oVAE-g">video</a> to get an intuition about eigen vectors</p>

<p>Eigen vectors are special vectors for a given matrix, which when multiplied by the matrix do not change their <a href="https://www.youtube.com/watch?v=k7RM-ot2NWY">span</a> but change their scale or magnitude</p>

<p>Eigenvalues is the scale by which an eigenvector changes when multiplied by a given matrix.</p>

<p>In other words, for an eigen vector of a matrix, multiplying the vector with the matrix is the same as multiplying the vector with a scalar. The scalar being the eigen value</p>

<p>This equation can be represented as:</p>

<p><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"><img class="special-img-class" src="/img/pca/eigen_vector_formula.png" /></a></p>

<blockquote>
<p><strong>Property</strong>: For a covariance matrix, the Eigenvector with the largest Eigenvalue points in the direction of maximal variance</p>
</blockquote>

<p>Eigenvectors and eigenvalues for the covariance matrices described above</p>

<p><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"><img class="special-img-class" src="/img/pca/eigen_vectors_cov_matrix2.png" /></a></p>

<p><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"><img class="special-img-class" src="/img/pca/eigen_vectors_cov_matrix1.png" /></a></p>

<blockquote>
<p>Given a set of eigen vectors for the covariance matrix, the principal components are the eigen vectors with the largets eigen values</p>
</blockquote>

  </div>
  

<div class="navigation navigation-single">
    
    
    <a href="/posts/pca/pca/" class="navigation-next">
      <span class="navigation-tittle">Principal Components Analysis - PCA Part 2/3</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    
<script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js" integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy"
  crossorigin="anonymous">
</script>

    
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
