<!DOCTYPE html>
<html lang="en-us">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.52" />

    
    
    

<title>Principal Components Analysis - PCA Part 2/3 • Datascope</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Principal Components Analysis - PCA Part 2/3"/>
<meta name="twitter:description" content="What is PCA? In simple words, PCA is a technique used to condense a large number of features into a smaller set of features that can almost represent the original set.
Why PCA?  PCA is used for dimensionality reduction.
 As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.
This is called &ldquo;Curse of dimensionality&rdquo;. Here is a video that explains this in simple terms."/>

<meta property="og:title" content="Principal Components Analysis - PCA Part 2/3" />
<meta property="og:description" content="What is PCA? In simple words, PCA is a technique used to condense a large number of features into a smaller set of features that can almost represent the original set.
Why PCA?  PCA is used for dimensionality reduction.
 As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.
This is called &ldquo;Curse of dimensionality&rdquo;. Here is a video that explains this in simple terms." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aparna-k.com/posts/pca/pca/" /><meta property="article:published_time" content="2017-03-11T19:59:42&#43;05:30"/>
<meta property="article:modified_time" content="2017-03-11T19:59:42&#43;05:30"/>


    






<link rel="stylesheet" href="/scss/hyde-hyde.7edd83059122cd386342a3b1c68b1e7b1b05b9634400072d2221148b98ef3449.css" integrity="sha256-ft2DBZEizThjQqOxxoseexsFuWNEAActIiEUi5jvNEk=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href=favicon.ico>
    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://aparna-k.com/">Datascope</a>
      </span>
      
      
      <p class="site__description">
         Prediction is risky, especially when it involves the future. - Niels Bohr 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Datascope</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		
		</li>
	</ul>
</div>

        <section class="social">
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/aparnakorattyswaroopam" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
</section>

      </div>
    </div>
    <div class="copyright">
  2017
</div>

  </div>
</div>

        <div class="content container">
            
    <article>
  <header>
    <h1>Principal Components Analysis - PCA Part 2/3</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Mar 11, 2017
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 4 min read
</div>


  </header>
  
  
  <div class="post">
    

<h2 id="what-is-pca">What is PCA?</h2>

<p>In simple words, PCA is a technique used to <em>condense</em> a large number of features into a smaller set of features that can <em>almost</em> represent the original set.</p>

<h2 id="why-pca">Why PCA?</h2>

<blockquote>
<p>PCA is used for dimensionality reduction.</p>
</blockquote>

<p>As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.</p>

<p>This is called <em>&ldquo;Curse of dimensionality&rdquo;</em>. <a href="https://www.youtube.com/watch?v=QZ0DtNFdDko">Here is a video</a> that explains this in simple terms.</p>

<p>PCA is a technique used to reduce diemnsions (or number of features).</p>

<h2 id="how-is-it-done">How is it done?</h2>

<p>For the sake of simplicity, consider a set of two features X = { X<sub>1</sub>, X<sub>2</sub> }</p>

<p>We are trying to find a line Z onto which we can project the points in the plane formed by { X<sub>1</sub>, X<sub>2</sub> }</p>

<p>Below is a reasonable visual representation of the idea</p>

<p><img class="special-img-class" style="max-width:400px; max-height:450px" src="https://liorpachter.files.wordpress.com/2014/05/pca_figure1.jpg?w=490&h=490" /></p>

<p>The line Z is the the 1D representation of the 2D dataset.</p>

<h2 id="what-are-principal-components">What are principal components?</h2>

<p>Similar to the line Z, there are other such lines, orthogonal to each other onto which points in the space can be projected.</p>

<p>Each of these lines capture the spatial information with varying degrees of accuracy.</p>

<p>In the image below we want to pick e to maximize variability. i.e., the <span style="color:#5f5fff"> <strong>purple</strong> </span> line captures the variability among points better than the <span style="color:green"> <strong>green</strong> </span> line</p>

<p><a href='http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/dim-2x2.pdf'><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/pca/PCA1.png" /></a></p>

<p>We want to do this, so that we reduce cases where points that are far apart in the original space are very close together on the projection.</p>

<p>The set of orthogonal lines that capture most amount of variability in the dataset are known as <strong>principal components</strong>.</p>

<h2 id="calculating-principal-components">Calculating principal components</h2>

<blockquote>
<p>Principal components are the eigen vectors of the covariance matrix of the dataset with the largest eigen values.</p>

<p>The eigenvector with the largest eigenvalue is the direction along which the data set has the maximum variance</p>
</blockquote>

<p>Refer to my previous post on <a href="https://aparna-k.com/posts/pca/covariance_eigen_pca/">covariance, eigen vectors and pca</a> for some background.</p>

<p>The technique of finding principal components is called <a href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca">&ldquo;Singular Value Decomposition (SVD)&rdquo;</a></p>

<p>However, we need not get into the details of SVD here, since for a symmetric matrix (which a covariance matrix is), <strong>singular values are just the absolute values of the eigenvalues of eigenvectors of a covariance matrix.</strong></p>

<h2 id="steps-involved-in-pca">Steps involved in PCA</h2>

<ol>
<li>Given a dataset, compute it&rsquo;s covariance matrix</li>
<li>Find the eigenvalues and eigenvectors for the covariance matrix</li>
<li>Sort the eigenvectors in descending order of their eigenvalues</li>
<li>Pick <strong>K</strong> eigenvectors from the sorted list (this set represents the maximum variance in data)</li>
</ol>

<h3 id="choosing-k-the-right-number-of-eigen-vectors">Choosing <strong>K</strong> (the right number of eigen vectors)</h3>

<p>Given a set of eigenvalues &lambda;<sub>1</sub>, &lambda;<sub>2</sub>, &lambda;<sub>3</sub> &hellip; &lambda;<sub>n</sub> in descending order of magnitude, pick &lambda;<sub>1</sub>, &lambda;<sub>2</sub>, &lambda;<sub>3</sub> &hellip; &lambda;<sub><b>K</b></sub> that <a href="https://ro-che.info/articles/2017-12-11-pca-explained-variance">&ldquo;explain the most variance&rdquo;</a></p>

<p>Pick <strong>K</strong> eigenvectors that explain 90% or more of the total variance, using the following equation</p>

<p><a href='http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/dim-2x2.pdf'><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/pca/explained_variance_pca.png" /></a></p>

<p>Typical threshold values are 0.9 or 0.95.
We could set a explained variance threshold as high as 0.99 if we want to capture the original data with higher accuracy in the compressed representation.</p>

<h2 id="reconstructing-data-from-pca">Reconstructing data from PCA:</h2>

<p>Let <strong>X<sub>raw</sub></strong> be the <em>n×p</em> data matrix with <em>n</em> rows (data points) and <em>p</em> columns (variables, or features). After subtracting the mean vector <em>μ</em> from each row, we get the centered data matrix <strong>X</strong>.</p>

<p>Let <strong>V</strong> be the <em>p×k</em> matrix of some <strong>K</strong> eigenvectors that we want to use; these would most often be the <strong>K</strong> eigenvectors with the largest eigenvalues. Then the <em>n×k</em> matrix of PCA projections (&ldquo;scores&rdquo;
will be simply given by <strong>Z=XV</strong>.</p>

<p>The reconstruction of the original data from Z is given by</p>

<p>X̂raw = ZVT + μ</p>

<h2 id="advice-for-applying-pca">Advice for applying PCA</h2>

<ol>
<li><p><strong>Perform feature scaling</strong> if the range of values for different features vary greatly.
Larger numbers are likely to have higher variance which can overwhelm the algorithm since we use PCs from the data&rsquo;s covariance matrix.</p></li>

<li><p><strong>Don&rsquo;t include PCA as a default step</strong> in an ML system design. Only apply PCA if the training doesn&rsquo;t work well with the original large feature set.</p></li>

<li><p><strong>PCA is a good way to reduce memory requirements</strong> and to speed up a learning algorithm</p></li>

<li><p><strong>PCA is not a good solution for overfitting</strong>. Instead of reducing the number of features using PCA, use regularization.</p></li>
</ol>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/posts/pca/covariance_eigen_pca/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Covariance, Eigen Vectors and Principal Components - PCA Part 1/3</span>
    </a>
    
    
    <a href="/posts/pca/pca_on_mnist/" class="navigation-next">
      <span class="navigation-tittle">PCA on MNIST using SKLearn - PCA Part 3/3</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    
<script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js" integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy"
  crossorigin="anonymous">
</script>

    
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
