<!DOCTYPE html>
<html lang="en-us">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.52" />

    
    
    

<title>Maximum Entropy Model - Intuition (Part 1/3) • Datascope</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Maximum Entropy Model - Intuition (Part 1/3)"/>
<meta name="twitter:description" content="Principle of Maximum Entropy  &ldquo;In making inferences on the basis of partial information we must use that probability distribution which has maximum entropy subject to whatever is known. This is the only unbiased assignment we can make; to use any other would amount to arbitrary assumption of information which,by hypothesis, we do not have&rdquo; - [Jaynes, 1957]
 If you are unfamiliar with the term information entropy, refer to my previous post"/>

<meta property="og:title" content="Maximum Entropy Model - Intuition (Part 1/3)" />
<meta property="og:description" content="Principle of Maximum Entropy  &ldquo;In making inferences on the basis of partial information we must use that probability distribution which has maximum entropy subject to whatever is known. This is the only unbiased assignment we can make; to use any other would amount to arbitrary assumption of information which,by hypothesis, we do not have&rdquo; - [Jaynes, 1957]
 If you are unfamiliar with the term information entropy, refer to my previous post" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aparna-k.com/posts/maxentmodelling/maxent-1/" /><meta property="article:published_time" content="2018-12-04T19:02:00&#43;05:30"/>
<meta property="article:modified_time" content="2018-12-04T19:02:00&#43;05:30"/>


    






<link rel="stylesheet" href="/scss/hyde-hyde.7edd83059122cd386342a3b1c68b1e7b1b05b9634400072d2221148b98ef3449.css" integrity="sha256-ft2DBZEizThjQqOxxoseexsFuWNEAActIiEUi5jvNEk=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href=favicon.ico>
    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://aparna-k.com/">Datascope</a>
      </span>
      
      
      <p class="site__description">
         Prediction is risky, especially when it involves the future. - Niels Bohr 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Datascope</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		
		</li>
	</ul>
</div>

        <section class="social">
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/aparnakorattyswaroopam" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
</section>

      </div>
    </div>
    <div class="copyright">
  2017
</div>

  </div>
</div>

        <div class="content container">
            
    <article>
  <header>
    <h1>Maximum Entropy Model - Intuition (Part 1/3)</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Dec 04, 2018
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 4 min read
</div>


  </header>
  
  
  <div class="post">
    

<h3 id="principle-of-maximum-entropy">Principle of Maximum Entropy</h3>

<blockquote>
<p>&ldquo;In making inferences on the basis of partial information we must use that probability distribution which has <em>maximum entropy</em> subject to whatever is known. This is the only <em>unbiased</em> assignment we can make; to use any other would amount to arbitrary assumption of information which,<strong><em>by hypothesis, we do not have</em></strong>&rdquo; - [Jaynes, 1957]</p>
</blockquote>

<p>If you are unfamiliar with the term <em>information entropy</em>, refer to <a href="https://aparna-k.com/posts/informationtheoryentropy/information_entropy/">my previous post</a></p>

<p>Say, you know that a person, Sally, has only shirts in 5 colours that she wears to work everyday.</p>

<p>The shirts are red, blue, green, yellow and pink in colour.</p>

<p>If this is all you knew and you were asked to <strong>calculate the probability</strong> that Sally would were a <em>pink</em> shirt on a given day, what would your response be?</p>

<p>I hope your answer was: 20% or <sup>1</sup>&frasl;<sub>5</sub></p>

<p>The reason that is the correct answer, is because, without any other information, all we can assume is that Sally wears any of the shirts on a given day <strong><em>with equal probability</em></strong></p>

<p>i.e., if $X$ is the random variable represnting the choice of colour on a given day, you would assign</p>

<p>p(X = red) = <sup>1</sup>&frasl;<sub>5</sub><br/>
p(X = blue) = <sup>1</sup>&frasl;<sub>5</sub><br/>
p(X = green) = <sup>1</sup>&frasl;<sub>5</sub><br/>
p(X = yellow) = <sup>1</sup>&frasl;<sub>5</sub><br/>
p(X = pink) = <sup>1</sup>&frasl;<sub>5</sub><br/></p>

<p>If you recall, from <a href="https://aparna-k.com/posts/informationtheoryentropy/information_entropy/">my previous post</a>, we know that</p>

<blockquote>
<p>Entropy is maximum when all probabilities are equally distributed</p>
</blockquote>

<p>Why? Because, entropy is a measure of <em>unpredictability</em> of a system, if all events in the system are equally likely, then the unpredicatbility is the highest</p>

<p>Intuitively, what we have done here by assigning equal probability to all outcomes is <em>follow the principle of maximum entropy</em></p>

<h3 id="maximum-entropy-model-motivating-example">Maximum Entropy Model - Motivating Example</h3>

<p>Say, we are building a English to French translator.</p>

<p>To do this, we collect a large sample of translations by an expert translator. We will use observations from this dataset to build our model.</p>

<p>Let&rsquo;s zoom into one small part of the problem, the translation of the word <em>in</em>.</p>

<p>To understand the best translation for the word <em>in</em> in French, we look through our dataset and come up with all possible translations of <em>in</em> to French.</p>

<p>We notice that, <em>in</em> has always been translated to one of the following five French phrases: <em>{ dans, en, à, au cours de, pendant }</em></p>

<p><strong>Representation</strong></p>

<p>$p(dans) = \frac{1}{5}$</p>

<p>Probability that the translation of <em>in</em> to French is <em>dans</em> is $\frac{1}{5}$</p>

<p><strong>Model</strong></p>

<p>If we have the fiven 5 words to choose from, then</p>

<blockquote>
<p>$p(dans) + p(en) + p(à) + p(au cours de) + p(pendant) = 1$</p>
</blockquote>

<p>This will be our first <em>constraint</em>, the model will have to always obey this.</p>

<p>There are infinite possibilities that obey this contatraint.</p>

<p><em>For Ex:</em>
  $p(pendant) = \frac{1}{2}$ &amp; $p(dans) = \frac{1}{2}$</p>

<p>obeys the constraint. But, we obviously cannot begin there.</p>

<p>We will always begin with probabilities that will <em>maximize entropy</em></p>

<p>i.e.,</p>

<p>$p(dans) = \frac{1}{5}$ <br/>
$p(en) = \frac{1}{5}$ <br/>
$p(à) = \frac{1}{5}$ <br/>
$p(au cours de) = \frac{1}{5}$ <br/>
$p(pendant) = \frac{1}{5}$ <br/></p>

<p>This model, which allocates the total probability evenly among the five possible phrases, is the most uniform model subject to our knowledge.</p>

<p>Say, we look through our data again and find that, approximately 30% of the time the translation for <em>in</em> was <em>dans</em> or <em>en</em></p>

<p>Now we have two constraints</p>

<blockquote>
<p>$p(dans) + p(en) + p(à) + p(au cours de) + p(pendant) = 1$ <br/>
$p(dans) + p(en) = \frac{1}{3}$</p>
</blockquote>

<p>Once again, of all the possible probability distributions we assume the one that has <em>maximum entropy</em></p>

<p>$p(dans) = \frac{3}{20}$ <br/>
$p(en) = \frac{3}{20}$ <br/>
$p(à) = \frac{7}{30}$ <br/>
$p(au cours de) = \frac{7}{30}$ <br/>
$p(pendant) = \frac{7}{30}$ <br/></p>

<p>Say, in another pass through the data we figure out that, in half the cases the translation of <em>in</em> was <em>dans</em> or <em>à</em></p>

<p>So, we now have 3 constraints</p>

<blockquote>
<p>$p(dans) + p(en) + p(à) + p(au cours de) + p(pendant) = 1$ <br/>
$p(dans) + p(en) = \frac{1}{3}$ <br/>
$p(dans) + p(à) = \frac{1}{2}$ <br/></p>
</blockquote>

<p>We can once again look for the most uniform distribution that satifies these constraints.</p>

<p>But, this time the choice is not as obvious. As we have added complexity, we have encountered two problems</p>

<ol>
<li><p>What exactly is meant by <em>uniform</em> and how can one measure the <em>uniformity</em> of the model?</p></li>

<li><p>How do we find a uniform model subject to the set of constraints like we have placed in the example above?</p></li>
</ol>

<p><em>Maxent model</em>  answers both these questions.
Intuitively, the principle is simple
&gt; Model all that is known, assume nothing about that which is unkown.</p>

<h3 id="references">References</h3>

<ol>
<li><p>Maxent Modelling Tutorial - <a href="https://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/node3.html">https://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/node3.html</a></p></li>

<li><p>A Simple Introduction to Maximum Entropy Models for Natural Language Processing - <a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&amp;context=ircs_reports">https://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&amp;context=ircs_reports</a></p></li>

<li><p>[Jaynes, 1957] Jaynes, E. T. (1957). Information Theory and Statistical Mechanics. <em>Physical Review</em>, 106:620-630</p></li>
</ol>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/posts/informationtheoryentropy/information_entropy/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">The idea of Information Entropy</span>
    </a>
    
    
    <a href="/posts/maxentmodelling/maxent-2/" class="navigation-next">
      <span class="navigation-tittle">Maximum Entropy Model - The details (Part 2/3)</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    
<script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js" integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy"
  crossorigin="anonymous">
</script>

    
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
