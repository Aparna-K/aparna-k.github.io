<!DOCTYPE html>
<html lang="en-us">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.52" />

    
    
    

<title>Maximum Entropy Model - The details (Part 2/3) • Datascope</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Maximum Entropy Model - The details (Part 2/3)"/>
<meta name="twitter:description" content="Model representation Output value: &gt; $y_i \in Y$ &gt; where $Y$ is a finite set ${y_1, y_2,&hellip;,y_n}$
Let&rsquo;s take for example, the translation model from my previous post
The task is to find the best translation of the word in to French. From the training data we infer that the translations of in to French typically is one of the following {dans, en, à, au cours de, pendant}
here, $Y = {dans, en, à, au cours de, pendant}$"/>

<meta property="og:title" content="Maximum Entropy Model - The details (Part 2/3)" />
<meta property="og:description" content="Model representation Output value: &gt; $y_i \in Y$ &gt; where $Y$ is a finite set ${y_1, y_2,&hellip;,y_n}$
Let&rsquo;s take for example, the translation model from my previous post
The task is to find the best translation of the word in to French. From the training data we infer that the translations of in to French typically is one of the following {dans, en, à, au cours de, pendant}
here, $Y = {dans, en, à, au cours de, pendant}$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aparna-k.com/posts/maxentmodelling/maxent-2/" /><meta property="article:published_time" content="2018-12-06T19:02:00&#43;05:30"/>
<meta property="article:modified_time" content="2018-12-06T19:02:00&#43;05:30"/>


    






<link rel="stylesheet" href="/scss/hyde-hyde.7edd83059122cd386342a3b1c68b1e7b1b05b9634400072d2221148b98ef3449.css" integrity="sha256-ft2DBZEizThjQqOxxoseexsFuWNEAActIiEUi5jvNEk=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href=favicon.ico>
    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://aparna-k.com/">Datascope</a>
      </span>
      
      
      <p class="site__description">
         Prediction is risky, especially when it involves the future. - Niels Bohr 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Datascope</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		
		</li>
	</ul>
</div>

        <section class="social">
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/aparnakorattyswaroopam" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
</section>

      </div>
    </div>
    <div class="copyright">
  2017
</div>

  </div>
</div>

        <div class="content container">
            
    <article>
  <header>
    <h1>Maximum Entropy Model - The details (Part 2/3)</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Dec 06, 2018
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 3 min read
</div>


  </header>
  
  
  <div class="post">
    

<h3 id="model-representation">Model representation</h3>

<p><strong>Output value:</strong>
&gt; $y_i \in Y$
&gt; where $Y$ is a finite set ${y_1, y_2,&hellip;,y_n}$</p>

<p>Let&rsquo;s take for example, the translation model from <a href="https://aparna-k.com/posts/maxentmodelling/maxent-1/">my previous post</a></p>

<p>The task is to find the best translation of the word <em>in</em> to French. From the training data we infer that the translations of <em>in</em> to French typically is one of the following
<em>{dans, en, à, au cours de, pendant}</em></p>

<p>here, $Y = {dans, en, à, au cours de, pendant}$</p>

<p><strong>Contextual Information:</strong></p>

<p>In generating the output $y$ there could be contextual information that influences the output, example, the words sorrounding <em>in</em> in a sentence.</p>

<p>This is represented as:</p>

<blockquote>
<p>$x_i \in X$
where $X$ is a finite set ${x_1, x_2,..,x_n}$</p>
</blockquote>

<p><strong>Empirical Probability Distribution:</strong></p>

<p>This is nothing but the probability that <em>you can count</em> in your training data.</p>

<p>Example: If event A occurs m times in your training data and there are n observations,  then the empirical probability of A is</p>

<p>$p̃(A) = \frac{m}{n}$</p>

<p>We take a large number of training samples $(x_1, y_1), (x_2, y_2)&hellip;,(x_N, y_N)$</p>

<p>and calculate the empirical probability as</p>

<p>$p̃(y_i, x_i) = \frac{number\ of\ times\ (y_i, x_i)\ occurs\ in\ the\ sample}{N}$</p>

<h4 id="features-and-constraints">Features and Constraints**</h4>

<p>Our goal is to construct a statistical model of the process which generated the training sample $p̃(y,x)$</p>

<p><strong>Feature Function/Feature</strong></p>

<p>This is a function that models the observation $y_i$ given some <em>context</em> $x_i$</p>

<p>Example:
Going back to the translation of <em>in</em>, we might observe in the training sample that <em>in</em> translates as <em>en</em> every time <em>April</em> follows <em>in</em></p>

<p>here the context $x_i$ is <em>April follows in</em> and the output $y_i$ is <em>en</em></p>

<p>$f(y_i, x_i) = \begin{cases}
              1\ if\ y = en\ and\ April\ follows\ in \\
              0\ otherwise \<br />
          \end{cases}$</p>

<p>The <a href="https://en.wikipedia.org/wiki/Expected_value#Finite_case">expected value</a> of $f(Y, X)$ w.r.t the <strong>empirical probability distribution</strong> $p̃(y_i, x_i)$ is given by:</p>

<blockquote>
<p>$p̃(f) = \sum_{x,y}p̃(y_i, x_i)f(y_i, x_i)$ &mdash;- (1)</p>
</blockquote>

<p>Say we compute a model $p(y|x)$
i.e, probability of output $y$ <a href="http://setosa.io/conditional/">condtioned</a> upon $x$</p>

<p>We represent the <em>feature function&rsquo;s</em> expected probability w.r.t the <strong>model $p(y|x)$</strong> as follows</p>

<blockquote>
<p>$p(f) = \sum_{x,y}p̃(x)p(y|x)f(y,x)$ &mdash;- (2)
where $p̃(x)$ is the <em>empirical probability</em> of $x$</p>
</blockquote>

<p><strong>Constraints</strong></p>

<p>$p(y,x)$ is the <em>joint probability</em> of x and y</p>

<p>$p(y|x)$ is the <a href="https://en.wikipedia.org/wiki/Conditional_probability"><em>conditional probability</em></a> of the output $y$ given a context $x$</p>

<p>By definition of conditional probability, we must have</p>

<p>$p(y,x) = p(y|x)p(x)$</p>

<p>Ideally, <em>empirical probability</em> and the <em>model</em> must match. i.e., the output of the <em>model</em> must be exactly the same as what we can measure</p>

<p>based on this assumption, we add the following contsraint</p>

<blockquote>
<p>$p̃(f) = p(f)$ &mdash;- (3)</p>
</blockquote>

<p><strong>Constraint Equation</strong></p>

<p>Combining (1), (2) and (3)</p>

<p>we write our <em>constraint equation</em> as</p>

<p>$ \sum_{x,y} p̃(x)p(y|x)f(y,x) = $ $ \sum p̃(x)p(y|x)f(y,x)$</p>

<p><strong>The Maxent Principle</strong></p>

<p>The <a href="https://en.wikipedia.org/wiki/Conditional_entropy">conditional entropy</a> $H(Y|X)$ is given as</p>

<p>$H(Y|X) = - \sum_{y_i,x_i} p(y_i, x_i)\ log\ p(y|x)$</p>

<p>Or</p>

<p>$H(Y|X) = - \sum_{y_i,x_i} p̃(x)p(y|x)\ log\ p(y|x)$</p>

<p>Following the <a href="https://aparna-k.com/posts/maxentmodelling/maxent-1/">principle of maximum entropy</a>, of all possible models $p(y|x)$ we choose the one with the maximum entropy</p>

<p>$p^* = \underset{p \in C}{\mathrm{argmax} H(p)}$</p>

<p>Where $C$ is a set of constraints $C = {c_1, c_2,&hellip;, c_n}$</p>

<h3 id="the-primal-problem">The primal problem</h3>

<p>The problem now becomes one of finding</p>

<p>$p^* = \underset{p \in C}{\mathrm{argmax} H(p)}$</p>

<p>Subject to the following constraints:</p>

<ol>
<li>$p(y|x) \ge 0\ for\ all\  x,y $</li>
<li>$\sum_{y}p(y|x) = 1\ for\ all\ x$</li>
<li>$ \sum_{x,y} p̃(x)p(y|x)f(y,x) = $ $ \sum p̃(x)p(y|x)f(y,x)$</li>
</ol>

<p>This <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction">constrained optimization</a> problem is solved using the Lagrangian</p>

<h3 id="the-equation-for-optimization">The equation for optimization</h3>

<p>The parametric form of the constrained optimization problem is</p>

<p>$p^*(y|X) = \frac{e^{\sum_{i}\lambda_if_i(y_i,x_i)}}{\sum_y e^{\sum_i\lambda_i f_i(y_i, x_i)}}$</p>

<p>Now we have the problem of finding value for parameter $\lambda$ that gives us the maximum value for y</p>

<h3 id="references">References</h3>

<ol>
<li><p>Maxent Modelling Tutorial - <a href="https://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/node3.html">https://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/node3.html</a></p></li>

<li><p>A Simple Introduction to Maximum Entropy Models for Natural Language Processing - <a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&amp;context=ircs_reports">https://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&amp;context=ircs_reports</a></p></li>

<li><p>[Jaynes, 1957] Jaynes, E. T. (1957). Information Theory and Statistical Mechanics. <em>Physical Review</em>, 106:620-630</p></li>

<li><p><a href="https://en.wikipedia.org/wiki/Expected_value#Finite_case">https://en.wikipedia.org/wiki/Expected_value#Finite_case</a></p></li>

<li><p><a href="https://people.eecs.berkeley.edu/~klein/papers/maxent-tutorial-slides.pdf">https://people.eecs.berkeley.edu/~klein/papers/maxent-tutorial-slides.pdf</a></p></li>
</ol>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/posts/maxentmodelling/maxent-1/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Maximum Entropy Model - Intuition (Part 1/3)</span>
    </a>
    
    
    <a href="/posts/nlp-pytorch/basics_nlp/" class="navigation-next">
      <span class="navigation-tittle">NLP - Feature representation</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    
<script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js" integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy"
  crossorigin="anonymous">
</script>

    
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
