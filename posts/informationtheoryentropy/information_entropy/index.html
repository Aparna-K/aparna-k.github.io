<!DOCTYPE html>
<html lang="en-us">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.52" />

    
    
    

<title>The idea of Information Entropy â€¢ Datascope</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The idea of Information Entropy"/>
<meta name="twitter:description" content="Information Entropy  Information entropy quantifies the average amount of information in an event.
 We could also think of entropy as a measure of unpredictability.
Information - Intuition Let&rsquo;s consider the following three sentences
 Tomorrow, the sun will rise from the East The phone will ring in the next one hour It will snow in Mumbai this winter  When we consider the amount of information in the three sentences above, there are two ways to think about it."/>

<meta property="og:title" content="The idea of Information Entropy" />
<meta property="og:description" content="Information Entropy  Information entropy quantifies the average amount of information in an event.
 We could also think of entropy as a measure of unpredictability.
Information - Intuition Let&rsquo;s consider the following three sentences
 Tomorrow, the sun will rise from the East The phone will ring in the next one hour It will snow in Mumbai this winter  When we consider the amount of information in the three sentences above, there are two ways to think about it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aparna-k.com/posts/informationtheoryentropy/information_entropy/" /><meta property="article:published_time" content="2018-12-01T19:02:42&#43;05:30"/>
<meta property="article:modified_time" content="2018-12-01T19:02:42&#43;05:30"/>


    






<link rel="stylesheet" href="/scss/hyde-hyde.7edd83059122cd386342a3b1c68b1e7b1b05b9634400072d2221148b98ef3449.css" integrity="sha256-ft2DBZEizThjQqOxxoseexsFuWNEAActIiEUi5jvNEk=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href=favicon.ico>
    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://aparna-k.com/">Datascope</a>
      </span>
      
      
      <p class="site__description">
         Prediction is risky, especially when it involves the future. - Niels Bohr 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Datascope</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		
		</li>
	</ul>
</div>

        <section class="social">
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/aparnakorattyswaroopam" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
</section>

      </div>
    </div>
    <div class="copyright">
  2017
</div>

  </div>
</div>

        <div class="content container">
            
    <article>
  <header>
    <h1>The idea of Information Entropy</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Dec 01, 2018
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 3 min read
</div>


  </header>
  
  
  <div class="post">
    

<h2 id="information-entropy">Information Entropy</h2>

<blockquote>
<p>Information entropy quantifies the average amount of information in an event.</p>
</blockquote>

<p>We could also think of entropy as a measure of <em>unpredictability</em>.</p>

<h3 id="information-intuition">Information - Intuition</h3>

<p>Let&rsquo;s consider the following three sentences</p>

<ol>
<li>Tomorrow, the sun will rise from the East</li>
<li>The phone will ring in the next one hour</li>
<li>It will snow in <a href="https://en.wikipedia.org/wiki/Mumbai">Mumbai</a> this winter</li>
</ol>

<p>When we consider the <em>amount of information</em> in the three sentences above, there are two ways to think about it.</p>

<ul>
<li><em>How many bits</em> do we require to transmit each sentence?</li>
<li><em>How much do we learn</em> from each sentence?</li>
</ul>

<p>The answers to these two questions don&rsquo;t necessarily have a relationship</p>

<p>The first sentence, <em>Tomorrow, the sun will rise from the East</em> probably requires the most amount of bits to transmit the information.</p>

<p>But, in terms of what we learn, does it really tell us anything useful?</p>

<blockquote>
<p>It doesn&rsquo;t. The sun will rise from the East tomorrow with a <em>probability of 100%</em> the sentence <em>adds no information</em></p>
</blockquote>

<p>The second sentence, <em>The phone will ring in the next one hour</em> - tells us something we wouldn&rsquo;t know.</p>

<blockquote>
<p>Without any information, all we can guess about a phone ringing in the next hour is that <em>the probability that the phone rings is 50</em>, the sentence changes this probability to a <em>100%</em>. Therefore, we can say that this sentence <em>adds some information</em></p>
</blockquote>

<p>If we know anything about Mumbai, we would predict a <em>0% probability</em> of snow</p>

<blockquote>
<p>When we read the third sentence, it&rsquo;s bound to raise some eyebrows. Nevertheless, it moves our estimate of probability from <em>0% to a 100</em>, that is adding a lot of information.</p>
</blockquote>

<p>The intuition, I hope is beginning to build now. We see that lower the probability of an event happening, the more information we gain from a sentence that changes our estimate of the event&rsquo;s probability.</p>

<p>Or, we could say, <em>Information</em> is <em>inversely proportional</em> to the <em>probability of that event</em></p>

<blockquote>
<p>$Information \propto \frac{1}{Probability\ of\ the\ event}$</p>
</blockquote>

<p>Formally, this is defined as:</p>

<blockquote>
<p>For a discrete random variable $X$ with possible outcomes $x_i, i=1,2,3,..,n$, the <em>Self Information</em> of the event $X=x_i$ is defined as:</p>

<p>$I(x_i) = \log{\frac{1}{p(x_i)}}$ $= -log\ p(x_i)$</p>

<p>Where, $p(x_i)$ is the probability that $X$ takes on the value $x_i$</p>
</blockquote>

<h3 id="information-entropy-1">Information Entropy</h3>

<p>Representation of Entropy $H(X)$</p>

<blockquote>
<p>$H(X) = -\sum_{x \in X} p(x)\log_2(p(x))$</p>
</blockquote>

<p>Or, in other words, this is the <em>mean</em> or <em>expectation</em> of Information</p>

<h4 id="intuition">Intuition</h4>

<p>Let&rsquo;s go back to the <em>plain English</em> definition of Entropy, i.e., &ldquo;Entropy is a measure of <em>unpredictability</em>.&rdquo;</p>

<p>An intuition for why this is true can be achieved by the following graph</p>

<p><img class="special-img-class" style="max-width:400px; max-height:450px" src="/img/information_entropy/Binary_entropy_plot.svg"/></p>

<p>This is the graph for entropy for random variable $X$ with two possible outcomes.</p>

<p>Like, a coin toss, where we only have two states</p>

<p>Say, we represent this as follows:</p>

<blockquote>
<p>$X = {1,\ if\ Head = True; 0\ if Head = False}$</p>
</blockquote>

<ul>
<li><p>If the coin is rigged,  and we know that a toss <strong><em>always</em></strong> results in a <em>head</em>, i.e., the probability $p(X = 1) = 1$</p></li>

<li><p>Also, $p(X = 0) = 0$</p></li>
</ul>

<p>Intuitively, if we already know this information, that the coin is rigged, there is <em>no uncertainity</em>, which means the entropy is 0</p>

<blockquote>
<p>$H(X) = - [ 1 \times \log_2(1) + 0 \times \log_2(0)] = 0$</p>
</blockquote>

<p>If we had a fair coin, we have an equal chance of any result, i.e,</p>

<p>$P(X = 1) = 0.5$
$P(X = 0) = 0.5$</p>

<p>Then,</p>

<blockquote>
<p>$H(X) = - [ 0.5 \times \log_2(0.5) + 0.5 \times \log_2(0.5)] = 1$</p>
</blockquote>

<p>This is the <strong>highest possible value for entropy</strong> for this system, in other words, <em>anything can happen with equal probability</em></p>

<h3 id="references">References:</h3>

<ol>
<li><p>NPTEL - Information Theory, Coding and Cryptography - <a href="https://nptel.ac.in/courses/108102117/1">https://nptel.ac.in/courses/108102117/1</a></p></li>

<li><p>Information Entropy - <a href="https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy">https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy</a></p></li>

<li><p><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory">https://en.wikipedia.org/wiki/Entropy_(information_theory</a>)</p></li>

<li><p><a href="http://micro.stanford.edu/~caiwei/me334/Chap7_Entropy_v04.pdf">http://micro.stanford.edu/~caiwei/me334/Chap7_Entropy_v04.pdf</a></p></li>

<li><p><a href="https://www.quora.com/Why-is-there-a-logarithmic-function-in-the-entropy-formula">https://www.quora.com/Why-is-there-a-logarithmic-function-in-the-entropy-formula</a></p></li>

<li><p><a href="https://revisionmaths.com/advanced-level-maths-revision/statistics/expectation-and-variance">https://revisionmaths.com/advanced-level-maths-revision/statistics/expectation-and-variance</a></p></li>
</ol>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/posts/pca/pca_on_mnist/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">PCA on MNIST using SKLearn - PCA Part 3/3</span>
    </a>
    
    
    <a href="/posts/maxentmodelling/maxent-1/" class="navigation-next">
      <span class="navigation-tittle">Maximum Entropy Model - Intuition (Part 1/3)</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    
<script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js" integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy"
  crossorigin="anonymous">
</script>

    
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
