<!DOCTYPE html>
<html lang="en-us">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.52" />

    
    
    

<title>The idea of Information Entropy â€¢ Datascope</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The idea of Information Entropy"/>
<meta name="twitter:description" content="Information Entropy  Information entropy quantifies how much information there is in an event.
 Information - Intuition Let&rsquo;s consider the following three sentences
 Tomorrow, the sun will rise from the East The phone will ring in the next one hour It will snow in Mumbai this winter  When we consider the amount of information in the three sentences above, there are two ways to think about it."/>

<meta property="og:title" content="The idea of Information Entropy" />
<meta property="og:description" content="Information Entropy  Information entropy quantifies how much information there is in an event.
 Information - Intuition Let&rsquo;s consider the following three sentences
 Tomorrow, the sun will rise from the East The phone will ring in the next one hour It will snow in Mumbai this winter  When we consider the amount of information in the three sentences above, there are two ways to think about it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aparna-k.com/posts/informationtheoryentropy/information_entropy/" /><meta property="article:published_time" content="2018-12-01T19:02:42&#43;05:30"/>
<meta property="article:modified_time" content="2018-12-01T19:02:42&#43;05:30"/>


    






<link rel="stylesheet" href="/scss/hyde-hyde.7edd83059122cd386342a3b1c68b1e7b1b05b9634400072d2221148b98ef3449.css" integrity="sha256-ft2DBZEizThjQqOxxoseexsFuWNEAActIiEUi5jvNEk=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://aparna-k.com/">Datascope</a>
      </span>
      
      
      <p class="site__description">
         Prediction is risky, especially when it involves the future. - Niels Bohr 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Datascope</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		
		</li>
	</ul>
</div>

        <section class="social">
	
	
	
	
	
	
	
	
	
	
	
	
	
	
</section>

      </div>
    </div>
    <div class="copyright">
  2017
</div>

  </div>
</div>

        <div class="content container">
            
    <article>
  <header>
    <h1>The idea of Information Entropy</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Dec 01, 2018
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 2 min read
</div>


  </header>
  
  
  <div class="post">
    

<h2 id="information-entropy">Information Entropy</h2>

<blockquote>
<p>Information entropy quantifies how much information there is in an event.</p>
</blockquote>

<h3 id="information-intuition">Information - Intuition</h3>

<p>Let&rsquo;s consider the following three sentences</p>

<ol>
<li>Tomorrow, the sun will rise from the East</li>
<li>The phone will ring in the next one hour</li>
<li>It will snow in <a href="https://en.wikipedia.org/wiki/Mumbai">Mumbai</a> this winter</li>
</ol>

<p>When we consider the <em>amount of information</em> in the three sentences above, there are two ways to think about it.</p>

<ul>
<li><em>How many bits</em> do we require to transmit each sentence?</li>
<li><em>How much do we learn</em> from each sentence?</li>
</ul>

<p>The answers to these two questions don&rsquo;t necessarily have a relationship</p>

<p>The first sentence, <em>Tomorrow, the sun will rise from the East</em> probably requires the most amount of bits to transmit the information.</p>

<p>But, in terms of what we learn, does it really tell us anything useful?</p>

<blockquote>
<p>It doesn&rsquo;t. The sun will rise from the East tomorrow with a <em>probability of 100%</em> the sentence <em>adds no information</em></p>
</blockquote>

<p>The second sentence, <em>The phone will ring in the next one hour</em> - tells us something we wouldn&rsquo;t know.</p>

<blockquote>
<p>Without any information, all we can guess about a phone ringing in the next hour is that <em>the probability that the phone rings is 50</em>, the sentence changes this probability to a <em>100%</em>. Therefore, we can say that this sentence <em>adds some information</em></p>
</blockquote>

<p>If we know anything about Mumbai, we would predict a <em>0% probability</em> of snow</p>

<blockquote>
<p>When we read the third sentence, it&rsquo;s bound to raise some eyebrows. Nevertheless, it moves our estimate of probability from <em>0% to a 100</em>, that is adding a lot of information.</p>
</blockquote>

<p>The intuition, I hope is beginning to build now. We see that lower the probability of an event happening, the more information we gain from a sentence that changes our estimate of the event&rsquo;s probability.</p>

<p>Or, we could say, <em>Information</em> is <em>inversely proportional</em> to the <em>probability of that event</em></p>

<blockquote>
<p>$Information \propto \frac{1}{Probability\ of\ the\ event}$</p>
</blockquote>

<p>Formally, this is defined as:</p>

<blockquote>
<p>For a discrete random variable $X$ with possible outcomes $x_i, i=1,2,3,..,n$, the <em>Self Information</em> of the event $X=x_i$ is defined as:</p>

<p>$I(x_i) = \log{\frac{1}{p(x_i)}}$ $= -log\ p(x_i)$</p>

<p>Where, $p(x_i)$ is the probability that $X$ takes on the value $x_i$</p>
</blockquote>

<h3 id="information-entropy-1">Information Entropy</h3>

<p>Representation of Entropy $H(X)$</p>

<blockquote>
<p>$H(X) = -\sum_{x \in X} p(x)\log(p(x))$</p>
</blockquote>

<p>Or, in other words, this is the <em>mean</em> or <em>expectation</em> of Information</p>

<h3 id="references">References:</h3>

<ol>
<li><p>NPTEL - Information Theory, Coding and Cryptography - <a href="https://nptel.ac.in/courses/108102117/1">https://nptel.ac.in/courses/108102117/1</a></p></li>

<li><p>Information Entropy - <a href="https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy">https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy</a></p></li>

<li><p><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory">https://en.wikipedia.org/wiki/Entropy_(information_theory</a>)</p></li>

<li><p><a href="http://micro.stanford.edu/~caiwei/me334/Chap7_Entropy_v04.pdf">http://micro.stanford.edu/~caiwei/me334/Chap7_Entropy_v04.pdf</a></p></li>

<li><p><a href="https://www.quora.com/Why-is-there-a-logarithmic-function-in-the-entropy-formula">https://www.quora.com/Why-is-there-a-logarithmic-function-in-the-entropy-formula</a></p></li>

<li><p><a href="https://revisionmaths.com/advanced-level-maths-revision/statistics/expectation-and-variance">https://revisionmaths.com/advanced-level-maths-revision/statistics/expectation-and-variance</a></p></li>
</ol>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/posts/pca/pca_on_mnist/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">PCA on MNIST using SKLearn - PCA Part 3/3</span>
    </a>
    
    
</div>


  

  
    


</article>


        </div>
        
    
<script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js" integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy"
  crossorigin="anonymous">
</script>

    
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
