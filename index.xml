<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Datascope</title>
    <link>https://aparna-k.com/</link>
    <description>Recent content on Datascope</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Mar 2017 17:32:42 +0530</lastBuildDate>
    
	<atom:link href="https://aparna-k.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PCA on MNIST using SKLearn - PCA Part 3/3</title>
      <link>https://aparna-k.com/posts/pca/pca_on_mnist/</link>
      <pubDate>Sun, 12 Mar 2017 17:32:42 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/pca/pca_on_mnist/</guid>
      <description>To understand basics of PCA, refer to my previous post here
Quick note regarding MNIST dataset: Ref: https://en.wikipedia.org/wiki/MNIST_database#Dataset
The MNIST dataset is a large database of handwritten digits that are commonly used for training and testing ML models.
The MNIST dataset in sklearn conatains 1797 digits. Each digit is represented by an array of 64 values that represent a 8x8 pixel image. Each pixel value ranges from 0-16.
import numpy as np import matplotlib.</description>
    </item>
    
    <item>
      <title>Principal Components Analysis - PCA Part 2/3</title>
      <link>https://aparna-k.com/posts/pca/pca/</link>
      <pubDate>Sat, 11 Mar 2017 19:59:42 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/pca/pca/</guid>
      <description>What is PCA? In simple words, PCA is a technique used to condense a large number of features into a smaller set of features that can almost represent the original set.
Why PCA?  PCA is used for dimensionality reduction.
 As the number of features increase in a dataset, the amount of data needed to draw meaningful insights increases exponentially.
This is called &amp;ldquo;Curse of dimensionality&amp;rdquo;. Here is a video that explains this in simple terms.</description>
    </item>
    
    <item>
      <title>Covariance, Eigen Vectors and Principal Components - PCA Part 1/3</title>
      <link>https://aparna-k.com/posts/pca/covariance_eigen_pca/</link>
      <pubDate>Fri, 10 Mar 2017 19:59:42 +0530</pubDate>
      
      <guid>https://aparna-k.com/posts/pca/covariance_eigen_pca/</guid>
      <description>Covariance matrix: Here is a really good reference to build an intuition for covariance matrix
Also known as the variance-covariance matrix, holds information about how two random variables (in this case two features) vary with respect to one another
Covariance is a measure of how much two random variables vary together. Itâ€™s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together.</description>
    </item>
    
  </channel>
</rss>